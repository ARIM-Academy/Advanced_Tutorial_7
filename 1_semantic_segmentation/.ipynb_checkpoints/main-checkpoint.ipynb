{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリの入力 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import splitfolders\n",
    "import shutil\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "from model import *\n",
    "from model2 import *\n",
    "from data import *\n",
    "from datasets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  ## KEYPOINT1\n",
    "\n",
    "log = open(\"training.log\", \"w\")\n",
    "sys.stdout = log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'training'   ## KEYPOINT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. データセットの準備\n",
    "モデル訓練や検証、テストに使用するデータを準備するための処理を行います。データをランダムに分割し、必要に応じて再分割することで、柔軟なデータセットの管理を可能にします。\n",
    "\n",
    "\n",
    "\n",
    "* データセットのディレクトリ構造やファイルは、以下のように想定されています：　data/microplastics/datasets_full_v2/\n",
    "* この中に分割対象のデータ（例: 画像ファイルやテキストファイル）が存在していると仮定しています。\n",
    "* mode という変数が 'training' または 'multiresunet_training' の場合に、このコードが実行されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == 'training' or mode == 'multiresunet_training':\n",
    "    \n",
    "    # shutil.rmtree('data/microplastics/train/')\n",
    "    # shutil.rmtree('data/microplastics/val/')\n",
    "    # shutil.rmtree('data/microplastics/test/')\n",
    "\n",
    "    splitfolders.ratio('./data/microplastics/datasets_full_v2', output=\"./data/microplastics/\", seed=2, ratio=(0.2, 0.2, 0.6))\n",
    "    shutil.move('./data/microplastics/train/', './data/microplastics/1/')\n",
    "    shutil.move('./data/microplastics/val/', './data/microplastics/2/')\n",
    "    shutil.move('./data/microplastics/test/', './data/microplastics/6/')\n",
    "    \n",
    "    splitfolders.ratio('data/microplastics/6/', output=\"data/microplastics/\", seed=2, ratio=(0.33, 0.33, 0.34))\n",
    "    shutil.move('./data/microplastics/train/', './data/microplastics/3/')\n",
    "    shutil.move('./data/microplastics/val/', './data/microplastics/4/')\n",
    "    shutil.move('./data/microplastics/test/', './data/microplastics/5/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training-testing spliting finished for 1/5 cross validation\n"
     ]
    }
   ],
   "source": [
    "crossval_i = 0\n",
    "\n",
    "if os.path.isdir('data/microplastics/train/') == True:\n",
    "    shutil.rmtree('data/microplastics/train/')\n",
    "    shutil.rmtree('data/microplastics/test/')\n",
    "\n",
    "os.mkdir('data/microplastics/train/')\n",
    "\n",
    "list_dir = ['data/microplastics/1/', 'data/microplastics/2/', 'data/microplastics/3/','data/microplastics/4/','data/microplastics/5/']\n",
    "\n",
    "copyTree(list_dir[crossval_i], 'data/microplastics/test/')\n",
    "\n",
    "del list_dir[crossval_i]\n",
    "\n",
    "for path in list_dir:\n",
    "    copyTree(path, 'data/microplastics/train/')\n",
    "\n",
    "print('training-testing spliting finished for {}/5 cross validation'.format(crossval_i+1))\n",
    "\n",
    "path_list_actual = os.listdir('data/microplastics/test/image/')\n",
    "testBatchSize = len(path_list_actual)\n",
    "\n",
    "data_gen_args = dict(rotation_range=0.2,\n",
    "            width_shift_range=0.05,\n",
    "            height_shift_range=0.05,\n",
    "            shear_range=0.05,\n",
    "            zoom_range=0.05,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest',\n",
    "            validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainGene = trainGenerator(2,'data/microplastics/train','image','label',data_gen_args,save_to_dir = None)\n",
    "validGene = validGenerator(2,'data/microplastics/train','image','label',data_gen_args,save_to_dir = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainStep_1(model, Train_generator, Valid_generator, epochs, batchSize, mode, pretrained_weights = None):\n",
    "    # the training phase\n",
    "    total_history_dict = dict()\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch : {}'.format(epoch + 1))\n",
    "\n",
    "        #model_checkpoint = ModelCheckpoint(pretrained_weights, monitor='loss', verbose=1, save_best_only=True)\n",
    "        history = model.fit(Train_generator, validation_data=Valid_generator, validation_steps=30, steps_per_epoch=300,epochs=1)  # the only para changed\n",
    "        \n",
    "        avgiou, avgdice_coef = evaluateStep(model, batchSize=batchSize, mode=mode)\n",
    "        avg_iou = [avgiou.numpy().tolist()]\n",
    "        avg_dice_coef = [avgdice_coef.numpy().tolist()]\n",
    "        for some_key in history.history.keys():  # save and append the results from each epoch\n",
    "            current_values = []\n",
    "            current_values += history.history[some_key]\n",
    "            if some_key in total_history_dict:\n",
    "                total_history_dict[some_key].append(current_values)\n",
    "            else:\n",
    "                total_history_dict[some_key] = [current_values]\n",
    "\n",
    "\n",
    "        if {'test_iou', 'test_dice_coef'} <= total_history_dict.keys():\n",
    "            total_history_dict['test_iou'].append(avg_iou)\n",
    "            total_history_dict['test_dice_coef'].append(avg_dice_coef)\n",
    "        else:\n",
    "            total_history_dict['test_iou'] = [avg_iou]\n",
    "            total_history_dict['test_dice_coef'] = [avg_dice_coef]\n",
    "    return total_history_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGenerator(batch_size, train_path, image_folder, mask_folder, aug_dict, \n",
    "                   image_color_mode=\"grayscale\", mask_color_mode=\"grayscale\", \n",
    "                   image_save_prefix=\"image\", mask_save_prefix=\"mask\", \n",
    "                   flag_multi_class=False, num_class=2, save_to_dir=None, \n",
    "                   target_size=(256, 256), seed=1):\n",
    "    \n",
    "    image_datagen = ImageDataGenerator(**aug_dict)\n",
    "    mask_datagen = ImageDataGenerator(**aug_dict)\n",
    "\n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes=[image_folder],\n",
    "        class_mode=None,\n",
    "        color_mode=image_color_mode,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        save_to_dir=save_to_dir,\n",
    "        save_prefix=image_save_prefix,\n",
    "        seed=seed,\n",
    "        subset='training'\n",
    "    )\n",
    "\n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes=[mask_folder],\n",
    "        class_mode=None,\n",
    "        color_mode=mask_color_mode,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        save_to_dir=save_to_dir,\n",
    "        save_prefix=mask_save_prefix,\n",
    "        seed=seed,\n",
    "        subset='training'\n",
    "    )\n",
    "\n",
    "    while True:  # Ensure continuous data generation\n",
    "        image_batch, mask_batch = next(zip(image_generator, mask_generator)) \n",
    "        img, mask = adjustData(image_batch, mask_batch, flag_multi_class, num_class)\n",
    "        yield (img, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 171 images belonging to 1 classes.\n",
      "Found 171 images belonging to 1 classes.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot take the length of shape with unknown rank.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m num_train_samples \u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(path_list_actual)\n\u001b[0;32m      4\u001b[0m model_checkpoint \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munet_microplastics.keras\u001b[39m\u001b[38;5;124m'\u001b[39m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 5\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainGene\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_train_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\japan\\anaconda3\\envs\\ARIM\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\japan\\anaconda3\\envs\\ARIM\\lib\\site-packages\\keras\\src\\metrics\\reduction_metrics.py:41\u001b[0m, in \u001b[0;36mreduce_to_samplewise_values\u001b[1;34m(values, sample_weight, reduce_fn, dtype)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weight_ndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     37\u001b[0m         sample_weight \u001b[38;5;241m=\u001b[39m reduce_fn(\n\u001b[0;32m     38\u001b[0m             sample_weight, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, weight_ndim))\n\u001b[0;32m     39\u001b[0m         )\n\u001b[1;32m---> 41\u001b[0m values_ndim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m values_ndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     43\u001b[0m     values \u001b[38;5;241m=\u001b[39m reduce_fn(values, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, values_ndim)))\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot take the length of shape with unknown rank."
     ]
    }
   ],
   "source": [
    "# Assuming you know the number of samples in trainGene (e.g., from data preprocessing)\n",
    "num_train_samples =len(path_list_actual)\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('unet_microplastics.keras', monitor='loss', verbose=1, save_best_only=True)\n",
    "history = model.fit(trainGene, steps_per_epoch=num_train_samples, epochs=10, callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_checkpoint = ModelCheckpoint('unet_microplastics.hdf5', monitor='loss', verbose=1, save_best_only=True)\n",
    "# history = model.fit_generator(trainGene,steps_per_epoch=3,epochs=10,callbacks=[model_checkpoint])\n",
    "## KEYPOINT4,5\n",
    "\n",
    "total_history_dict = trainStep_1(model, trainGene, validGene, \n",
    "                            epochs=5, batchSize=testBatchSize, \n",
    "                            mode=mode, \n",
    "                            pretrained_weights='unet_microplastics_{}.keras'.format(crossval_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_history_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(total_history_dict['accuracy'], 'b-')\n",
    "plt.plot(total_history_dict['iou'], 'g-')\n",
    "plt.plot(total_history_dict['dice_coef'], 'r-')\n",
    "plt.plot(total_history_dict['test_iou'], 'g--')\n",
    "plt.plot(total_history_dict['test_dice_coef'], 'r--')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Metrics')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['accuracy', 'iou', 'dice_coef','test_iou', 'test_dice_coef'], loc='upper left')\n",
    "# plt.show()\n",
    "plt.grid()\n",
    "plt.savefig('training_unet_{}.png'.format(crossval_i))\n",
    "plt.close()\n",
    "a_file = open(\"data_unet_{}.pkl\".format(crossval_i), \"wb\")\n",
    "pickle.dump(total_history_dict, a_file)\n",
    "a_file.close()\n",
    "# a_file = open(\"data{}.pkl\".format(crossval_i), \"rb\")\n",
    "# output = pickle.load(a_file)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解説　splitfolders.ratio()\n",
    "\n",
    "splitfolders ライブラリを使用して、データセットを指定した比率で分割します。\n",
    "* 入力: 'data/microplastics/datasets_full_v2'（データ元）\n",
    "* 出力: 'data/microplastics/'（分割されたデータの保存先）\n",
    "* 比率: 訓練用 20%、検証用 20%、テスト用 60%\n",
    "* ランダム性: seed=2 を指定して、データ分割の再現性を保証。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for crossval_i in range(5):    ## KEYPOINT3\n",
    "    if os.path.isdir('data/microplastics/train/') == True:\n",
    "        shutil.rmtree('data/microplastics/train/')\n",
    "        shutil.rmtree('data/microplastics/test/')\n",
    "    os.mkdir('data/microplastics/train/')\n",
    "\n",
    "    list_dir = ['data/microplastics/1/', 'data/microplastics/2/', 'data/microplastics/3/','data/microplastics/4/','data/microplastics/5/']\n",
    "\n",
    "    copyTree(list_dir[crossval_i], 'data/microplastics/test/')\n",
    "    del list_dir[crossval_i]\n",
    "    \n",
    "    for path in list_dir:\n",
    "        copyTree(path, 'data/microplastics/train/')\n",
    "\n",
    "    print('training-testing spliting finished for {}/5 cross validation'.format(crossval_i+1))\n",
    "    print(mode)\n",
    "    \n",
    "    path_list_actual = os.listdir('data/microplastics/test/image/')\n",
    "    testBatchSize = len(path_list_actual)\n",
    "\n",
    "\n",
    "    data_gen_args = dict(rotation_range=0.2,\n",
    "                        width_shift_range=0.05,\n",
    "                        height_shift_range=0.05,\n",
    "                        shear_range=0.05,\n",
    "                        zoom_range=0.05,\n",
    "                        horizontal_flip=True,\n",
    "                        fill_mode='nearest',\n",
    "                        validation_split=0.1)\n",
    "    \n",
    "    trainGene = trainGenerator(2,'data/microplastics/train','image','label',data_gen_args,save_to_dir = None)\n",
    "    validGene = validGenerator(2,'data/microplastics/train','image','label',data_gen_args,save_to_dir = None)\n",
    "\n",
    "    print ('OK')\n",
    "\n",
    "\n",
    "    if mode == 'training':\n",
    "        model = unet()\n",
    "        # model_checkpoint = ModelCheckpoint('unet_microplastics.hdf5', monitor='loss', verbose=1, save_best_only=True)\n",
    "        # history = model.fit_generator(trainGene,steps_per_epoch=3,epochs=10,callbacks=[model_checkpoint])\n",
    "        ## KEYPOINT4,5\n",
    "\n",
    "        total_history_dict = trainStep(model, trainGene, validGene, \n",
    "                                       epochs=100, batchSize=testBatchSize, \n",
    "                                       mode=mode, \n",
    "                                       pretrained_weights='unet_microplastics_{}.keras'.format(crossval_i))\n",
    "        \n",
    "        plt.plot(total_history_dict['accuracy'], 'b-')\n",
    "        plt.plot(total_history_dict['iou'], 'g-')\n",
    "        plt.plot(total_history_dict['dice_coef'], 'r-')\n",
    "        plt.plot(total_history_dict['test_iou'], 'g--')\n",
    "        plt.plot(total_history_dict['test_dice_coef'], 'r--')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.ylabel('Metrics')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['accuracy', 'iou', 'dice_coef','test_iou', 'test_dice_coef'], loc='upper left')\n",
    "        # plt.show()\n",
    "        plt.grid()\n",
    "        plt.savefig('training_unet_{}.png'.format(crossval_i))\n",
    "        plt.close()\n",
    "        a_file = open(\"data_unet_{}.pkl\".format(crossval_i), \"wb\")\n",
    "        pickle.dump(total_history_dict, a_file)\n",
    "        a_file.close()\n",
    "        # a_file = open(\"data{}.pkl\".format(crossval_i), \"rb\")\n",
    "        # output = pickle.load(a_file)\n",
    "        # print(output)\n",
    "\n",
    "    elif mode == 'testing':\n",
    "        model = unet('unet_microplastics.hdf5')\n",
    "        \n",
    "    elif mode == 'multiresunet_training':\n",
    "        model = MultiResUnet()\n",
    "        # model_checkpoint = ModelCheckpoint('unet_microplastics.hdf5', monitor='loss', verbose=1, save_best_only=True)\n",
    "        # history = model.fit_generator(trainGene,steps_per_epoch=3,epochs=10,callbacks=[model_checkpoint])\n",
    "        total_history_dict = trainStep(model, trainGene, validGene, epochs=100, batchSize=testBatchSize, mode=mode, pretrained_weights='multiresunet_microplastics_{}.hdf5'.format(crossval_i))\n",
    "        plt.plot(total_history_dict['accuracy'], 'b-')\n",
    "        plt.plot(total_history_dict['iou'], 'g-')\n",
    "        plt.plot(total_history_dict['dice_coef'], 'r-')\n",
    "        plt.plot(total_history_dict['test_iou'], 'g--')\n",
    "        plt.plot(total_history_dict['test_dice_coef'], 'r--')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.ylabel('Metrics')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['accuracy', 'iou', 'dice_coef','test_iou', 'test_dice_coef'], loc='upper left')\n",
    "        # plt.show()\n",
    "        plt.grid()\n",
    "        plt.savefig('training_multiresunet_{}.png'.format(crossval_i))\n",
    "        plt.close()\n",
    "        a_file = open(\"data_multiresunet_{}.pkl\".format(crossval_i), \"wb\")\n",
    "        pickle.dump(total_history_dict, a_file)\n",
    "        a_file.close()\n",
    "\n",
    "    elif mode == 'multiresunet_testing':\n",
    "        model = MultiResUnet('multiresunet_microplastics.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# results = model.predict(testGene,batch_size=30, verbose=1)\n",
    "# results = testResize(\"data/microplastics/test/image\", results, 30, flag_resize=True)\n",
    "# results = (results>0.5)\n",
    "#\n",
    "# if mode == 'training' or mode == 'testing':\n",
    "#     saveResult(\"data/microplastics/test/image\", \"data/microplastics/result\", results)\n",
    "#     evaluateResult(\"data/microplastics/result\", \"data/microplastics/test/label\", 30)\n",
    "#     print('*************')\n",
    "# elif mode == 'multiresunet_training' or mode  == 'multiresunet_testing':\n",
    "#     saveResult(\"data/microplastics/test/image\", \"data/microplastics/result2\", results)\n",
    "#     evaluateResult(\"data/microplastics/result2\", \"data/microplastics/test/label\", 30)\n",
    "#     print('-------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ARIM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
