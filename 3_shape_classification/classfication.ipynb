{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator  # 修正箇所\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from livelossplot.inputs.keras import PlotLossesCallback\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import splitfolders\n",
    "import shutil\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "from datasets import copyTree\n",
    "\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = open(\"training.log\", \"w\")\n",
    "sys.stdout = log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, n_classes, optimizer='rmsprop', fine_tune=0):\n",
    "    \"\"\"\n",
    "    Compiles a model integrated with VGG16 pretrained layers\n",
    "\n",
    "    input_shape: tuple - the shape of input images (width, height, channels)\n",
    "    n_classes: int - number of classes for the output layer\n",
    "    optimizer: string - instantiated optimizer to use for training. Defaults to 'RMSProp'\n",
    "    fine_tune: int - The number of pre-trained layers to unfreeze.\n",
    "                If set to 0, all pretrained layers will freeze during training\n",
    "    \"\"\"\n",
    "\n",
    "    # Pretrained convolutional layers are loaded using the Imagenet weights.\n",
    "    # Include_top is set to False, in order to exclude the model's fully-connected layers.\n",
    "    conv_base = VGG16(include_top=False,\n",
    "                      weights='imagenet',\n",
    "                      input_shape=input_shape)\n",
    "\n",
    "    # Defines how many layers to freeze during training.\n",
    "    # Layers in the convolutional base are switched from trainable to non-trainable\n",
    "    # depending on the size of the fine-tuning parameter.\n",
    "    if fine_tune > 0:\n",
    "        for layer in conv_base.layers[:-fine_tune]:\n",
    "            layer.trainable = False\n",
    "    else:\n",
    "        for layer in conv_base.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "    # Create a new 'top' of the model (i.e. fully-connected layers).\n",
    "    # This is 'bootstrapping' a new top_model onto the pretrained layers.\n",
    "    top_model = conv_base.output\n",
    "    top_model = Flatten(name=\"flatten\")(top_model)\n",
    "    # top_model = Dense(4096, activation='relu')(top_model)\n",
    "    # top_model = Dense(1072, activation='relu')(top_model)\n",
    "    top_model = Dense(64, activation='relu')(top_model)\n",
    "    top_model = Dense(32, activation='relu')(top_model)\n",
    "    top_model = Dropout(0.2)(top_model)\n",
    "    output_layer = Dense(n_classes, activation='softmax')(top_model)\n",
    "\n",
    "    # Group the convolutional base and new fully-connected layers into a Model object.\n",
    "    model = Model(inputs=conv_base.input, outputs=output_layer)\n",
    "\n",
    "    # Compiles the model for training.\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(y_true, y_pred, class_names, ax, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        square=True,\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names,\n",
    "        fmt='d',\n",
    "        cmap=plt.cm.Blues,\n",
    "        cbar=False,\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "    ax.set_ylabel('True Label', fontsize=12)\n",
    "    ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "#\n",
    "# def f1_score(y_true, y_pred):\n",
    "#     y_true = K.cast(y_true, dtype = 'float32')\n",
    "#     y_pred = K.cast(y_pred, dtype = 'float32')\n",
    "#     # Count positive samples.\n",
    "#     c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "#     c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "#     c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "#     # If there are no true samples, fix the F1 score at 0.\n",
    "#     if c3 == 0:\n",
    "#         return 0.\n",
    "#     # How many selected items are relevant?\n",
    "#     precision = 1. * c1 / c2\n",
    "#     # How many relevant items are selected?\n",
    "#     recall = 1. * c1 / c3\n",
    "#     # Calculate f1_score\n",
    "#     f1_score = 2. * (precision * recall) / (precision + recall)\n",
    "#     return f1_score\n",
    "#\n",
    "# def precision(y_true, y_pred):\n",
    "#     y_true = K.cast(y_true, dtype = 'float32')\n",
    "#     y_pred = K.cast(y_pred, dtype = 'float32')\n",
    "#     # Count positive samples.\n",
    "#     c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "#     c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "#     c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "#     # If there are no true samples, fix the F1 score at 0.\n",
    "#     if c3 == 0:\n",
    "#         return 0.\n",
    "#     # How many selected items are relevant?\n",
    "#     precision = 1. * c1 / c2\n",
    "#     return precision\n",
    "#\n",
    "# def recall(y_true, y_pred):\n",
    "#     y_true = K.cast(y_true, dtype = 'float32')\n",
    "#     y_pred = K.cast(y_pred, dtype = 'float32')\n",
    "#     # Count positive samples.\n",
    "#     c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "#     c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "#     c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "#     # If there are no true samples, fix the F1 score at 0.\n",
    "#     if c3 == 0:\n",
    "#         return 0.\n",
    "#     # How many relevant items are selected?\n",
    "#     recall = 1. * c1 / c3\n",
    "#     return recall\n",
    "\n",
    "\n",
    "# scratch_model = load_model('s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'copyTree' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m os\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/microplastics/train/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m list_dir \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/microplastics/1/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/microplastics/2/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/microplastics/3/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/microplastics/4/\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/microplastics/5/\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 9\u001b[0m \u001b[43mcopyTree\u001b[49m(list_dir[crossval_i], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/microplastics/test/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m list_dir[crossval_i]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m list_dir:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'copyTree' is not defined"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "mode = '5-cross'\n",
    "if mode == '5-cross':\n",
    "    print('good')\n",
    "    # shutil.rmtree('data/microplastics/train/')\n",
    "    # shutil.rmtree('data/microplastics/val/')\n",
    "    # shutil.rmtree('data/microplastics/test/')\n",
    "    splitfolders.ratio('data/microplastics/images', output=\"data/microplastics/\", seed=2, ratio=(0.2, 0.2, 0.6))\n",
    "    shutil.move('data/microplastics/train/', 'data/microplastics/1/')\n",
    "    shutil.move('data/microplastics/val/', 'data/microplastics/2/')\n",
    "    shutil.move('data/microplastics/test/', 'data/microplastics/6/')\n",
    "    splitfolders.ratio('data/microplastics/6/', output=\"data/microplastics/\", seed=2, ratio=(0.33, 0.33, 0.34))\n",
    "    shutil.move('data/microplastics/train/', 'data/microplastics/3/')\n",
    "    shutil.move('data/microplastics/val/', 'data/microplastics/4/')\n",
    "    shutil.move('data/microplastics/test/', 'data/microplastics/5/')\n",
    "\n",
    "for crossval_i in range(5):  ## KEYPOINT3\n",
    "    if os.path.isdir('data/microplastics/train/') == True:\n",
    "        shutil.rmtree('data/microplastics/train/')\n",
    "        shutil.rmtree('data/microplastics/test/')\n",
    "    os.mkdir('data/microplastics/train/')\n",
    "\n",
    "    list_dir = ['data/microplastics/1/', 'data/microplastics/2/', 'data/microplastics/3/', 'data/microplastics/4/',\n",
    "                'data/microplastics/5/']\n",
    "    copyTree(list_dir[crossval_i], 'data/microplastics/test/')\n",
    "    del list_dir[crossval_i]\n",
    "    for path in list_dir:\n",
    "        copyTree(path, 'data/microplastics/train/')\n",
    "    print('training-testing spliting finished for {}/5 cross validation'.format(crossval_i + 1))\n",
    "    print(mode)\n",
    "\n",
    "    train_generator = ImageDataGenerator(rotation_range=90,\n",
    "                                         width_shift_range=0.2,\n",
    "                                         height_shift_range=0.2,\n",
    "                                         shear_range=0.2,\n",
    "                                         zoom_range=0.2,\n",
    "                                         horizontal_flip=True,\n",
    "                                         vertical_flip=True,\n",
    "                                         validation_split=0.2,\n",
    "                                         fill_mode='reflect',\n",
    "                                         preprocessing_function=preprocess_input)\n",
    "\n",
    "    test_generator = ImageDataGenerator(preprocessing_function=preprocess_input)  # VGG16 preprocessing\n",
    "\n",
    "    train_data_dir = 'data/microplastics/train'\n",
    "    test_data_dir = 'data/microplastics/test'\n",
    "\n",
    "    class_subset = sorted(os.listdir('data/microplastics/images'))[:3]  # Using only the first 3 classes\n",
    "\n",
    "    traingen = train_generator.flow_from_directory(train_data_dir,\n",
    "                                                   target_size=(224, 224),\n",
    "                                                   class_mode='categorical',\n",
    "                                                   classes=class_subset,\n",
    "                                                   subset='training',\n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   shuffle=True,\n",
    "                                                   seed=5)\n",
    "\n",
    "    validgen = train_generator.flow_from_directory(train_data_dir,\n",
    "                                                   target_size=(224, 224),\n",
    "                                                   class_mode='categorical',\n",
    "                                                   classes=class_subset,\n",
    "                                                   subset='validation',\n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   shuffle=True,\n",
    "                                                   seed=5)\n",
    "\n",
    "    testgen = test_generator.flow_from_directory(test_data_dir,\n",
    "                                                 target_size=(224, 224),\n",
    "                                                 class_mode=None,\n",
    "                                                 classes=class_subset,\n",
    "                                                 batch_size=1,\n",
    "                                                 shuffle=False,\n",
    "                                                 seed=5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    input_shape = (224, 224, 3)\n",
    "    optim_1 = Adam(learning_rate=0.001)\n",
    "    n_classes = 3\n",
    "\n",
    "    # n_steps = 5 * traingen.samples // BATCH_SIZE\n",
    "    # n_val_steps = 5 * validgen.samples // BATCH_SIZE\n",
    "    n_steps = 1 * traingen.n // BATCH_SIZE\n",
    "    n_val_steps = 1 * validgen.n// BATCH_SIZE\n",
    "    n_epochs = 50\n",
    "\n",
    "    # First we'll train the model without Fine-tuning\n",
    "    # vgg_model = create_model(input_shape, n_classes, optim_1, fine_tune=0)\n",
    "    # plot_loss_1 = PlotLossesCallback()\n",
    "\n",
    "    # Use a smaller learning rate\n",
    "    optim_2 = Adam(lr=0.0001)\n",
    "    # Re-compile the model, this time leaving the last 2 layers unfrozen for Fine-Tuning\n",
    "    vgg_model_ft = create_model(input_shape, n_classes, optim_2, fine_tune=2)\n",
    "\n",
    "    plot_loss_2 = PlotLossesCallback()\n",
    "    # ModelCheckpoint callback - save best weights\n",
    "    tl_checkpoint_1 = ModelCheckpoint(filepath='tl_model_v1.weights.{}.best.hdf5'.format(crossval_i),\n",
    "                                      save_best_only=True,\n",
    "                                      verbose=1)\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='loss',\n",
    "                               patience=5,\n",
    "                               restore_best_weights=True,\n",
    "                               mode='min')\n",
    "\n",
    "    # Retrain model with fine-tuning\n",
    "    vgg_ft_history = vgg_model_ft.fit(traingen,\n",
    "                                      batch_size=BATCH_SIZE,\n",
    "                                      epochs=n_epochs,\n",
    "                                      validation_data=validgen,\n",
    "                                      steps_per_epoch=n_steps,\n",
    "                                      validation_steps=n_val_steps,\n",
    "                                      callbacks=[tl_checkpoint_1, early_stop, plot_loss_2],\n",
    "                                      # callbacks=[tl_checkpoint_1, plot_loss_2],\n",
    "                                      verbose=1)\n",
    "\n",
    "    # Generate predictions\n",
    "    vgg_model_ft.load_weights('tl_model_v1.weights.{}.best.hdf5'.format(crossval_i)) # initialize the best trained weights\n",
    "    true_classes = testgen.classes\n",
    "    class_indices = traingen.class_indices\n",
    "    class_indices = dict((v,k) for k,v in class_indices.items())\n",
    "\n",
    "    vgg_preds_ft = vgg_model_ft.predict(testgen)\n",
    "    vgg_pred_classes_ft = np.argmax(vgg_preds_ft, axis=1)\n",
    "\n",
    "    print('**1:', vgg_pred_classes_ft, type(vgg_pred_classes_ft))\n",
    "    print('**2:', true_classes, type(true_classes))\n",
    "    vgg_acc_ft = accuracy_score(true_classes, vgg_pred_classes_ft)\n",
    "    print(\"VGG16 Model Accuracy with Fine-Tuning: {:.2f}%\".format(vgg_acc_ft * 100))\n",
    "    print(\"precision\", precision_score(true_classes, vgg_pred_classes_ft, average='weighted')*100,\n",
    "          \"recall\", recall_score(true_classes, vgg_pred_classes_ft, average='weighted')*100,\n",
    "          \"f1_score\", f1_score(true_classes, vgg_pred_classes_ft, average='weighted')*100)\n",
    "\n",
    "\n",
    "    class_names = testgen.class_indices.keys()\n",
    "\n",
    "\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plot_heatmap(true_classes, vgg_pred_classes_ft, class_names, ax, title=\"Transfer Learning (VGG16) with Fine-Tuning\")\n",
    "\n",
    "    # fig.suptitle(\"Confusion Matrix Model Comparison\", fontsize=24)\n",
    "    # fig.tight_layout()\n",
    "    # fig.subplots_adjust(top=1.25)\n",
    "    # plt.show()\n",
    "    plt.savefig('confusion_matrix_{}.png'.format(crossval_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ARIM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
